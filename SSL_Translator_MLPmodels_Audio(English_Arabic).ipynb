{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "700771dc",
   "metadata": {},
   "source": [
    "\n",
    "# Saudi Sign Gesture Recognition System\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This project is a **Saudi Sign Gesture Recognition System** that detects hand gestures and converts them into corresponding speech using the **Google Cloud Text-to-Speech API**. The system recognizes both **sign language gestures** (using one hand) and **numbers** (using two hands). The detected gestures or numbers are then converted into audio, and the system plays the spoken output in English/Arabic.\n",
    "\n",
    "The core of the system relies on **MediaPipe** for hand landmark detection, **pre-trained machine learning models** for recognizing gestures and numbers, and Google’s **Text-to-Speech (TTS)** service for converting the recognized gestures/numbers into spoken words. Both the sign and number models are trained using **multilayer perceptrons** on a large dataset and have undergone **hyperparameter tuning** to optimize performance.\n",
    "\n",
    "## How the System Works\n",
    "\n",
    "1. **Hand Detection and Tracking**:\n",
    "   - The system captures real-time video from a webcam using **OpenCV**.\n",
    "   - It uses **MediaPipe** to detect hand landmarks and track hand movements. It works with either one or two hands at a time.\n",
    "\n",
    "2. **Gesture and Number Recognition**:\n",
    "   - For **sign language gestures**: If only one hand is detected, the system predicts the hand gesture using a pre-trained model (`finalized_model_hyp_onlysigns.sav`), which classifies the gesture based on hand landmarks.Some preprocessing are carried out before passing the data to the model.\n",
    "   - For **numbers**: If two hands are detected, the system uses another pre-trained model (`numbers_model_iter2.sav`) to predict the number represented by the hand signs.\n",
    "\n",
    "3. **Text-to-Speech Conversion**:\n",
    "   - Once a gesture or number is recognized and held for 5 seconds, the corresponding text is passed to the Google Cloud TTS API, which synthesizes speech in English/Arabic.\n",
    "   - The synthesized audio is saved as an MP3 file and played back through the speakers.\n",
    "\n",
    "This system is a powerful tool for enhancing communication through gesture recognition and speech synthesis, making it particularly useful for educational and assistive technologies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5416ad61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number '1' detected for 5 seconds. Saving and playing sound.\n",
      "Audio content saved to response_1.mp3\n",
      "Played response_1.mp3\n",
      "Number '2' detected for 5 seconds. Saving and playing sound.\n",
      "Audio content saved to response_2.mp3\n",
      "Played response_2.mp3\n",
      "Number '3' detected for 5 seconds. Saving and playing sound.\n",
      "Audio content saved to response_3.mp3\n",
      "Played response_3.mp3\n"
     ]
    }
   ],
   "source": [
    "#Reference: https://github.com/kinivi/hand-gesture-recognition-mediapipe\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import copy\n",
    "import itertools\n",
    "import csv\n",
    "import playsound\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from google.cloud import texttospeech\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "import io\n",
    "import threading\n",
    "import time\n",
    "from playsound import playsound\n",
    "\n",
    "class HandGestureRecognition:\n",
    "    def __init__(self):\n",
    "        # Initialize mediapipe\n",
    "        self.mp_drawing = mp.solutions.drawing_utils\n",
    "        self.mp_hands = mp.solutions.hands\n",
    "\n",
    "        # Load pre-trained models for gesture recognition\n",
    "        self.model_signs = pickle.load(open('model/finalized_model_hyp_onlysigns.sav', 'rb'))\n",
    "        self.model_numbers = pickle.load(open('model/numbers_model_iter2.sav', 'rb'))\n",
    "\n",
    "        # Text-to-speech client initialization\n",
    "        os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = 'text-to-speech-435015-82e32527fb00.json'\n",
    "        self.client = texttospeech.TextToSpeechClient()\n",
    "\n",
    "        # Text-to-speech parameters\n",
    "        self.voice = texttospeech.VoiceSelectionParams(\n",
    "            language_code=\"ar-XA\", ssml_gender=texttospeech.SsmlVoiceGender.NEUTRAL)\n",
    "        self.audio_config = texttospeech.AudioConfig(\n",
    "            audio_encoding=texttospeech.AudioEncoding.MP3)\n",
    "        \n",
    "        \n",
    "#         # Dictionary for number-to-english word mapping - need to work on accessing this\n",
    "#         self.sign_to_word = {\n",
    "#                     \"Alif\": 'أ', \"Ba\": 'ب', \"Ta\": 'ت', \"Tha\": 'ث', \"Jim\": 'ج', \"Ha\": 'ح', \"Kha\": 'خ',\n",
    "#                     \"Dal\": 'د', \"Zay\": 'ز', \"Sien\": 'س', \"Shien\": 'ش', \"Sad\": 'ص', \"Dhad\": 'ض',\n",
    "#                     \"Tah\": 'ط', \"Thah\": 'ظ', \"Ayn\": 'ع', \"Ghayn\": 'غ', \"Fa\": 'ف', \"Qaf\": 'ق',\n",
    "#                     \"Kaf\": 'ك', \"Lam\": 'ل', \"Miem\": 'م', \"Noon\": 'ن', \"He\": 'هـ', \"Waw\": 'و',\n",
    "#                     \"Taa\": 'ت', \"La\": 'لا'}\n",
    "\n",
    "        # Dictionary for number-to-english word mapping\n",
    "        self.num_to_word = {\n",
    "            1: 'one', 2: 'two', 3: 'three', 4: 'four',\n",
    "            5: 'five', 6: 'six', 7: 'seven', 8: 'eight',\n",
    "            9: 'nine', 10: 'ten'\n",
    "        }\n",
    "        \n",
    "        #Dictionary for number-to-arabic word mapping\n",
    "#         self.num_to_word = { \"1\": '١', \"2\": '٢', \"3\": '٣', \"4\": '٤', \"5\": '٥', \n",
    "#                             \"6\": '٦', \"7\": '٧', \"8\": '٨', \"9\": '٩', \"10\": '١٠'}\n",
    "\n",
    "        # Global states for gesture tracking and speech synthesis\n",
    "        self.prev_result = None\n",
    "        self.response = None\n",
    "        self.lock = threading.Lock()\n",
    "        self.prev_time = time.time()\n",
    "        self.sign_count = 0\n",
    "        self.same_gesture_duration = 0\n",
    "\n",
    "    def calc_landmark_list(self, image, landmarks):\n",
    "        \"\"\"Calculate the hand landmarks in pixel coordinates.\"\"\"\n",
    "        image_width, image_height = image.shape[1], image.shape[0]\n",
    "        landmark_point = []\n",
    "        for _, landmark in enumerate(landmarks.landmark):\n",
    "            landmark_x = min(int(landmark.x * image_width), image_width - 1)\n",
    "            landmark_y = min(int(landmark.y * image_height), image_height - 1)\n",
    "            landmark_point.append([landmark_x, landmark_y])\n",
    "        return landmark_point\n",
    "\n",
    "    def pre_process_landmark(self, landmark_list):\n",
    "        \"\"\"Normalize the hand landmarks and flatten into a single list.\"\"\"\n",
    "        base_x, base_y = landmark_list[0][0], landmark_list[0][1]\n",
    "        for index, point in enumerate(landmark_list):\n",
    "            landmark_list[index][0] -= base_x\n",
    "            landmark_list[index][1] -= base_y\n",
    "        landmark_list = [coord for point in landmark_list for coord in point]\n",
    "        max_value = max(list(map(abs, landmark_list)))\n",
    "        return [n / max_value for n in landmark_list]\n",
    "\n",
    "    def text_to_speech(self, text):\n",
    "        \"\"\"Convert text to speech, save audio incrementally, and play it.\"\"\"\n",
    "        # Convert text (numpy.int64) to string if it's a number\n",
    "        if isinstance(text, np.int64):\n",
    "            text = str(self.num_to_word.get(int(text), text))\n",
    "\n",
    "        synthesis_input = texttospeech.SynthesisInput(text=text)\n",
    "\n",
    "        # Retry mechanism for API call\n",
    "        retries = 3\n",
    "        while retries > 0:\n",
    "            try:\n",
    "                with self.lock:\n",
    "                    self.response = self.client.synthesize_speech(\n",
    "                        input=synthesis_input,\n",
    "                        voice=self.voice,\n",
    "                        audio_config=self.audio_config\n",
    "                    )\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Text-to-speech error: {e}, retrying...\")\n",
    "                retries -= 1\n",
    "                if retries == 0:\n",
    "                    print(\"Failed after 3 retries.\")\n",
    "                    return\n",
    "\n",
    "        # Increment the sign count to keep track of responses\n",
    "        self.sign_count += 1\n",
    "\n",
    "        # Save the synthesized speech to a file incrementally\n",
    "        audio_file = f\"response_{self.sign_count}.mp3\"\n",
    "        try:\n",
    "            # Writing the synthesized speech content to a file\n",
    "            with open(audio_file, 'wb') as out:\n",
    "                out.write(self.response.audio_content)\n",
    "\n",
    "            print(f\"Audio content saved to {audio_file}\")\n",
    "\n",
    "            # Ensure a small delay to avoid potential file access issues\n",
    "            time.sleep(0.5)\n",
    "\n",
    "            # Play the saved audio file using playsound\n",
    "            playsound(audio_file)\n",
    "            print(f\"Played {audio_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to save or play {audio_file}: {e}\")\n",
    "\n",
    "    def detect_hands(self):\n",
    "        \"\"\"Main method to detect hand gestures and trigger actions based on recognized gestures.\"\"\"\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        with self.mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7, max_num_hands=2) as hands:\n",
    "            while cap.isOpened():\n",
    "                success, image = cap.read()\n",
    "                if not success:\n",
    "                    print(\"Ignoring empty camera frame.\")\n",
    "                    continue\n",
    "\n",
    "                # Flip the image for a selfie view and convert color to RGB\n",
    "                image = cv2.flip(image, 1)\n",
    "                image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                # Process the image for hand landmarks\n",
    "                results = hands.process(image_rgb)\n",
    "                debug_image = copy.deepcopy(image)\n",
    "\n",
    "                if results.multi_hand_landmarks:\n",
    "                    if len(results.multi_hand_landmarks) == 1:\n",
    "                        # One hand detected, use the model_signs for gesture prediction (sign language)\n",
    "                        hand_landmarks = results.multi_hand_landmarks[0]\n",
    "                        landmark_list = self.calc_landmark_list(debug_image, hand_landmarks)\n",
    "                        pre_processed_landmark_list = self.pre_process_landmark(landmark_list)\n",
    "\n",
    "                        # Predict gesture using the pre-trained model for signs\n",
    "                        result = self.model_signs.predict([pre_processed_landmark_list])[0]\n",
    "\n",
    "                        # Compare scalars instead of arrays to avoid warning\n",
    "                        if result != self.prev_result:\n",
    "                            self.same_gesture_duration = time.time() - self.prev_time\n",
    "                            self.prev_time = time.time()\n",
    "                        else:\n",
    "                            self.same_gesture_duration = time.time() - self.prev_time\n",
    "\n",
    "                        # Display gesture on the frame\n",
    "                        cv2.putText(image, str(result), (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "                        # If the gesture stays the same for 5 seconds, trigger text-to-speech and save it\n",
    "                        if self.same_gesture_duration >= 5:\n",
    "                            print(f\"Gesture '{result}' detected for 5 seconds. Saving and playing sound.\")\n",
    "                            threading.Thread(target=self.text_to_speech, args=(result,)).start()\n",
    "                            self.prev_time = time.time()\n",
    "\n",
    "                        self.prev_result = result\n",
    "\n",
    "                    elif len(results.multi_hand_landmarks) == 2:\n",
    "                        # Two hands detected, use the model_numbers for number prediction\n",
    "                        left_hand = results.multi_hand_landmarks[0]\n",
    "                        right_hand = results.multi_hand_landmarks[1]\n",
    "\n",
    "                        # Calculate and normalize landmarks for both hands\n",
    "                        left_landmark_list = self.calc_landmark_list(debug_image, left_hand)\n",
    "                        right_landmark_list = self.calc_landmark_list(debug_image, right_hand)\n",
    "\n",
    "                        # Pre-process both hand landmark lists\n",
    "                        left_pre_processed = self.pre_process_landmark(left_landmark_list)\n",
    "                        right_pre_processed = self.pre_process_landmark(right_landmark_list)\n",
    "\n",
    "                        # Combine both hands landmarks for model input\n",
    "                        combined_landmarks = left_pre_processed + right_pre_processed\n",
    "\n",
    "                        # Predict numbers using the pre-trained model for numbers\n",
    "                        result = self.model_numbers.predict([combined_landmarks])[0]\n",
    "\n",
    "                        # Convert number result to corresponding text before passing to TTS\n",
    "                        result_text = self.num_to_word.get(int(result), str(result))\n",
    "\n",
    "                        # Compare scalars instead of arrays to avoid warning\n",
    "                        if result != self.prev_result:\n",
    "                            self.same_gesture_duration = time.time() - self.prev_time\n",
    "                            self.prev_time = time.time()\n",
    "                        else:\n",
    "                            self.same_gesture_duration = time.time() - self.prev_time\n",
    "\n",
    "                        # Display the number prediction on the frame\n",
    "                        cv2.putText(image, str(result), (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "                        # If the gesture stays the same for 5 seconds, trigger text-to-speech and save it\n",
    "                        if self.same_gesture_duration >= 5:\n",
    "                            print(f\"Number '{result_text}' detected for 5 seconds. Saving and playing sound.\")\n",
    "                            threading.Thread(target=self.text_to_speech, args=(result_text,)).start()\n",
    "                            self.prev_time = time.time()\n",
    "\n",
    "                        self.prev_result = result\n",
    "\n",
    "                    # Draw landmarks for all hands detected\n",
    "                    for hand_landmarks in results.multi_hand_landmarks:\n",
    "                        self.mp_drawing.draw_landmarks(image, hand_landmarks, mp.solutions.hands.HAND_CONNECTIONS)\n",
    "\n",
    "                # Display the image with detected landmarks\n",
    "                cv2.imshow('Hand Gesture Recognition', image)\n",
    "\n",
    "                # Exit if the Esc key is pressed\n",
    "                if cv2.waitKey(1) & 0xFF == 27:  # 27 is the ASCII code for Esc\n",
    "                    break\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    hand_gesture_recognition = HandGestureRecognition()\n",
    "    hand_gesture_recognition.detect_hands()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fad6f97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
